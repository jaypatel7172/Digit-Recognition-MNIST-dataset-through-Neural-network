{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from mlxtend.data import loadlocal_mnist\n",
    "from activations import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "import cv2 \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":                                    #if you want to use relu for hidden layers\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    #For hidden layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    #Output layer    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "Al,cache = L_model_forward(train_x,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    \n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":                    #if you want to use relu for hidden layers\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,\"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMNIST( prefix, folder ):\n",
    "    intType = np.dtype( 'int32' ).newbyteorder( '>' )\n",
    "    nMetaDataBytes = 4 * intType.itemsize\n",
    "\n",
    "    data = np.fromfile( folder + \"/\" + prefix + '-images-idx3-ubyte', dtype = 'ubyte' )\n",
    "    magicBytes, nImages, width, height = np.frombuffer( data[:nMetaDataBytes].tobytes(), intType )\n",
    "    data = data[nMetaDataBytes:].astype( dtype = 'float32' ).reshape( [ nImages, width, height ] )\n",
    "\n",
    "    labels = np.fromfile( folder + \"/\" + prefix + '-labels-idx1-ubyte',\n",
    "                          dtype = 'ubyte' )[2 * intType.itemsize:]\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter path to the folder where images are stored in second argument\n",
    "train_x_orig, train_y = loadMNIST( \"train\", \"/Users/jay/Documents/Neural Networks/Digits/data/\" )\n",
    "test_x_orig, test_y = loadMNIST( \"t10k\", \"/Users/jay/Documents/Neural Networks/Digits/data/\" )\n",
    "\n",
    "train_y = train_y[0:64]\n",
    "test_y = train_y[0:32]\n",
    "train_y = train_y.reshape(1,64)\n",
    "test_y = test_y.reshape(1,32)\n",
    "\n",
    "train_x_orig = train_x_orig[0:64]\n",
    "test_x_orig = train_x_orig[0:32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is:6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 13\n",
    "plt.imshow(train_x_orig[index])\n",
    "print(\"The number is:\" + str(train_y[0,index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 64\n",
      "Number of testing examples: 32\n",
      "Each image is of size: (28, 28, 3)\n",
      "train_x_orig shape: (64, 28, 28)\n",
      "train_y shape: (1, 64)\n",
      "test_x_orig shape: (32, 28, 28)\n",
      "test_y shape: (1, 32)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 64)\n",
      "test_x's shape: (784, 32)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    c = j.shape[1]\n",
    "    e = np.zeros((10, c))\n",
    "    y = 0\n",
    "    for column in j.T:\n",
    "        x = int(column)\n",
    "        e[x][y] = 1.0\n",
    "        y+=1\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y shape: (10, 64)\n",
      "test_y shape: (10, 32)\n"
     ]
    }
   ],
   "source": [
    "train_y = vectorize(train_y)\n",
    "test_y = vectorize(test_y)\n",
    "print(\"train_y shape: \" + str(train_y.shape))\n",
    "print(\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [784,30,10] #  2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "AL,cache = L_model_forward(train_x,parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.01, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.931644\n",
      "Cost after iteration 100: 3.329711\n",
      "Cost after iteration 200: 3.275494\n",
      "Cost after iteration 300: 3.195975\n",
      "Cost after iteration 400: 3.025316\n",
      "Cost after iteration 500: 2.752480\n",
      "Cost after iteration 600: 2.414859\n",
      "Cost after iteration 700: 2.044862\n",
      "Cost after iteration 800: 1.706922\n",
      "Cost after iteration 900: 1.429908\n",
      "Cost after iteration 1000: 1.202115\n",
      "Cost after iteration 1100: 1.008675\n",
      "Cost after iteration 1200: 0.842268\n",
      "Cost after iteration 1300: 0.701609\n",
      "Cost after iteration 1400: 0.584001\n",
      "Cost after iteration 1500: 0.486212\n",
      "Cost after iteration 1600: 0.406111\n",
      "Cost after iteration 1700: 0.341120\n",
      "Cost after iteration 1800: 0.288531\n",
      "Cost after iteration 1900: 0.246158\n",
      "Cost after iteration 2000: 0.211872\n",
      "Cost after iteration 2100: 0.184049\n",
      "Cost after iteration 2200: 0.161363\n",
      "Cost after iteration 2300: 0.142762\n",
      "Cost after iteration 2400: 0.127379\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X,Y,parameters):\n",
    "    AL, caches = L_model_forward(X, parameters)    \n",
    "    # Compute cost.\n",
    "    cost = compute_cost(AL, Y)\n",
    "    return cost,AL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of test data: 0.12680513797263268\n"
     ]
    }
   ],
   "source": [
    "cost , AL = test(test_x,test_y,parameters)\n",
    "print(\"Cost of test data: \"+str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(my_image,parameters):\n",
    "    AL, caches = L_model_forward(my_image, parameters)\n",
    "    res = np.max(AL,axis=0)\n",
    "    prediction = np.where(AL == res)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts number to be [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_image = \"image2.jpeg\" #Experiment images can be found in images folder \n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(cv2.imread(fname, 0))\n",
    "my_image = misc.imresize(image, size=(28,28))/255\n",
    "my_imagef = my_image.reshape(28*28,1)\n",
    "plt.imshow(image)\n",
    "prediction= predict(my_imagef, parameters)\n",
    "print(\"The model predicts number to be \" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
