{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from mlxtend.data import loadlocal_mnist\n",
    "from activations import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "import cv2 \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":                                    #if you want to use relu for hidden layers\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    #For hidden layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    #Output layer    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al,cache = L_model_forward(train_x,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    \n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":                    #if you want to use relu for hidden layers\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,\"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMNIST( prefix, folder ):\n",
    "    intType = np.dtype( 'int32' ).newbyteorder( '>' )\n",
    "    nMetaDataBytes = 4 * intType.itemsize\n",
    "\n",
    "    data = np.fromfile( folder + \"/\" + prefix + '-images-idx3-ubyte', dtype = 'ubyte' )\n",
    "    magicBytes, nImages, width, height = np.frombuffer( data[:nMetaDataBytes].tobytes(), intType )\n",
    "    data = data[nMetaDataBytes:].astype( dtype = 'float32' ).reshape( [ nImages, width, height ] )\n",
    "\n",
    "    labels = np.fromfile( folder + \"/\" + prefix + '-labels-idx1-ubyte',\n",
    "                          dtype = 'ubyte' )[2 * intType.itemsize:]\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter path to the folder where images are stored in second argument\n",
    "train_x_orig, train_y = loadMNIST( \"train\", \"/Users/jay/Documents/Neural Networks/Digits/data/\" )\n",
    "test_x_orig, test_y = loadMNIST( \"t10k\", \"/Users/jay/Documents/Neural Networks/Digits/data/\" )\n",
    "\n",
    "train_y = train_y[0:500]\n",
    "test_y = train_y[0:50]\n",
    "train_y = train_y.reshape(1,500)\n",
    "test_y = test_y.reshape(1,50)\n",
    "\n",
    "train_x_orig = train_x_orig[0:500]\n",
    "test_x_orig = train_x_orig[0:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is:6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAN9ElEQVR4nO3db4xVdX7H8c9HuhujEIGaTogg0A2YbBrLViIkhUpDdkMxEdcHKzwwmprMmqwGFW1x+2CNTRPSYjXxwRrImqVmcYOKwRB1sUikarJx/FNB7a5TBXYGhCDGZTVmFb59MIdmxLm/O97/zvf9SiZz7/nO75xvrnw8555zz/05IgRg4jun2w0A6AzCDiRB2IEkCDuQBGEHkviTTm7MNqf+gTaLCI+1vKk9u+0Vtn9je9D2+mbWBaC93Oh1dtuTJP1W0nclDUl6WdKaiHirMIY9O9Bm7dizXy5pMCLejYg/SvqlpFVNrA9AGzUT9osk/W7U86Fq2RfY7rc9YHugiW0BaFLbT9BFxCZJmyQO44FuambPPixp1qjnM6tlAHpQM2F/WdI823Ntf1PSaklPtqYtAK3W8GF8RHxu+2ZJv5I0SdJDEfFmyzoD0FINX3praGO8Zwfari0fqgHw9UHYgSQIO5AEYQeSIOxAEoQdSKKj97Mjn/nz59esPfPMM8WxkyZNKtZnz57dUE9ZsWcHkiDsQBKEHUiCsANJEHYgCcIOJMGlNzTlgQceKNavvfbamrXp06cXx+7cubOhnjA29uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATfLptcX19fsb59+/ZiffHixcV66d/X/v37i2OXL19erH/wwQfFelZ8uyyQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMH97BNc6aucJWnjxo3F+qJFi5ra/l133VWzNjAwUBzLdfTWairstg9IOinplKTPI2JhK5oC0Hqt2LP/bUQcb8F6ALQR79mBJJoNe0jaZfsV2/1j/YHtftsDtstv0AC0VbOH8UsiYtj2n0l61vb/RMTe0X8QEZskbZK4EQbopqb27BExXP0+JukJSZe3oikArddw2G2fb3vKmceSviepfM8igK5p5jC+T9ITts+sZ2tElOfgRcfV+272lStXtnX7Q0NDNWt79uxp67bxRQ2HPSLelfSXLewFQBtx6Q1IgrADSRB2IAnCDiRB2IEkuMV1Aijdxrp169bi2OrSacOuueaaYn3Hjh1NrR+tw54dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgOvsEcN1119WsXXzxxcWxTz31VLF+0003FevDw8PFOnoHe3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSMIRnZukhRlhGvPSSy8V6wsWLKhZO3z4cHHsihUrivXBwcFiHb0nIsb8kgL27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBPez94BVq1YV64sWLSrWS5+VePTRR4tjP/3002IdE0fdPbvth2wfs71/1LLptp+1/U71e1p72wTQrPEcxv9c0tkfs1ovaXdEzJO0u3oOoIfVDXtE7JV04qzFqyRtqR5vkXR1i/sC0GKNvmfvi4gj1eP3JfXV+kPb/ZL6G9wOgBZp+gRdRETpBpeI2CRpk8SNMEA3NXrp7ajtGZJU/T7WupYAtEOjYX9S0vXV4+slMS8v0OPqHsbbfkTSMkkX2h6S9BNJGyRts32jpIOSftDOJr/upk6dWqwvXbq0bdv+8MMPi/WhoaG2bbuetWvXFuuzZs1qav133HFHU+Mnmrphj4g1NUrLW9wLgDbi47JAEoQdSIKwA0kQdiAJwg4kwS2uHXDq1Kli/bLLLivWzzmn/P/k06dP16zt3bu3OLZZt912W8Njb7nllmJ99uzZDa9bktatW1ezNnPmzOLYiTgVNXt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC6+wdcMUVVxTr9W5xLV1Hl6RDhw7VrB0/frw4tp7SdNBS/d6vuuqqhrf98ccfF+v1bs+95JJLatYee+yx4tjVq1cX6wcPHizWexF7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IguvsLTBlypRife7cuU2t//Dhw8X6ww8/XLM2ODhYHDt//vxi/c477yzW6003XbrOv2vXruLYe++9t1i/4IILivXnnnuu4bETEXt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC6+wtsGTJkmL9vvvua2r9mzdvLtbvueeemrW+vr7i2I0bNxbrK1euLNZPnjxZrG/btq1mrd6UyvPmzSvWH3zwwWK91Nvu3buLY7+O96vXU3fPbvsh28ds7x+17G7bw7Zfr37K/yIAdN14DuN/LmnFGMvvi4gF1c9TrW0LQKvVDXtE7JV0ogO9AGijZk7Q3Wz7jeowf1qtP7Ldb3vA9kAT2wLQpEbD/lNJ35K0QNIRSTXvWIiITRGxMCIWNrgtAC3QUNgj4mhEnIqI05I2S7q8tW0BaLWGwm57xqin35e0v9bfAugNda+z235E0jJJF9oekvQTSctsL5AUkg5I+mEbe+x5l156aVvXX7qOXs/27duL9UWLFjW8bqn+/ezPP/98zdrixYuLY1944YWGejrj/vvvr1mrd41/Iqob9ohYM8bin7WhFwBtxMdlgSQIO5AEYQeSIOxAEoQdSIJbXFtg6tSpxbrtYn3Hjh1Nbb80rfKcOXOKY+v1tm7dumK9dGlNKn9V9datW4tjm+2tdOktI/bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE19k7ICKaqjfj9OnTTW273u27hw4dKtbPPffcmrX33nuvOHbp0qXF+kcffVSs44vYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEm7nNd4vbczu3MY6qN1fiVxvSujS/ewbNmwojp08eXJDPZ1R757z48eP16zdcMMNxbFPP/10Iy2lFxFj/kdhzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXA/ewt89tlnxfonn3xSrJ933nnF+osvvlisd/KzEmc7efJksb5t27aaNa6jd1bdPbvtWbb32H7L9pu211bLp9t+1vY71e9p7W8XQKPGcxj/uaR1EfFtSYsl/cj2tyWtl7Q7IuZJ2l09B9Cj6oY9Io5ExKvV45OS3pZ0kaRVkrZUf7ZF0tXtahJA877Se3bbcyR9R9KvJfVFxJGq9L6kvhpj+iX1N94igFYY99l425MlPS7p1oj4/ehajJwhGvMsUURsioiFEbGwqU4BNGVcYbf9DY0E/RcRsb1afNT2jKo+Q9Kx9rQIoBXq3uLqkXsYt0g6ERG3jlr+b5I+iIgNttdLmh4R/1BnXRPyFtd6rrzyymL99ttvL9aXLVtWrDdz6W3Lli3F+r59+4r11157rVivN6UzWq/WLa7jec/+15Kuk7TP9uvVsh9L2iBpm+0bJR2U9INWNAqgPeqGPSJekFTrGwqWt7YdAO3Cx2WBJAg7kARhB5Ig7EAShB1Igq+SBiYYvkoaSI6wA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSqBt227Ns77H9lu03ba+tlt9te9j269XPyva3C6BRdSeJsD1D0oyIeNX2FEmvSLpaI/Ox/yEiNo57Y0wSAbRdrUkixjM/+xFJR6rHJ22/Lemi1rYHoN2+0nt223MkfUfSr6tFN9t+w/ZDtqfVGNNve8D2QFOdAmjKuOd6sz1Z0vOS/iUittvuk3RcUkj6Z40c6v99nXVwGA+0Wa3D+HGF3fY3JO2U9KuI+Pcx6nMk7YyIv6izHsIOtFnDEzvatqSfSXp7dNCrE3dnfF/S/mabBNA+4zkbv0TSf0naJ+l0tfjHktZIWqCRw/gDkn5YncwrrYs9O9BmTR3GtwphB9qP+dmB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ1P3CyRY7LungqOcXVst6Ua/21qt9SfTWqFb2NrtWoaP3s39p4/ZARCzsWgMFvdpbr/Yl0VujOtUbh/FAEoQdSKLbYd/U5e2X9GpvvdqXRG+N6khvXX3PDqBzur1nB9AhhB1Ioitht73C9m9sD9pe340earF9wPa+ahrqrs5PV82hd8z2/lHLptt+1vY71e8x59jrUm89MY13YZrxrr523Z7+vOPv2W1PkvRbSd+VNCTpZUlrIuKtjjZSg+0DkhZGRNc/gGH7byT9QdJ/nJlay/a/SjoRERuq/1FOi4h/7JHe7tZXnMa7Tb3Vmmb8BnXxtWvl9OeN6Mae/XJJgxHxbkT8UdIvJa3qQh89LyL2Sjpx1uJVkrZUj7do5B9Lx9XorSdExJGIeLV6fFLSmWnGu/raFfrqiG6E/SJJvxv1fEi9Nd97SNpl+xXb/d1uZgx9o6bZel9SXzebGUPdabw76axpxnvmtWtk+vNmcYLuy5ZExF9J+jtJP6oOV3tSjLwH66Vrpz+V9C2NzAF4RNK93Wymmmb8cUm3RsTvR9e6+dqN0VdHXrduhH1Y0qxRz2dWy3pCRAxXv49JekIjbzt6ydEzM+hWv491uZ//FxFHI+JURJyWtFldfO2qacYfl/SLiNheLe76azdWX5163boR9pclzbM91/Y3Ja2W9GQX+vgS2+dXJ05k+3xJ31PvTUX9pKTrq8fXS9rRxV6+oFem8a41zbi6/Np1ffrziOj4j6SVGjkj/7+S/qkbPdTo688l/Xf182a3e5P0iEYO6z7TyLmNGyX9qaTdkt6R9J+SpvdQbw9rZGrvNzQSrBld6m2JRg7R35D0evWzstuvXaGvjrxufFwWSIITdEAShB1IgrADSRB2IAnCDiRB2IEkCDuQxP8BtqxlXZlDlJMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 13\n",
    "plt.imshow(train_x_orig[index])\n",
    "print(\"The number is:\" + str(train_y[0,index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 500\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (28, 28, 3)\n",
      "train_x_orig shape: (500, 28, 28)\n",
      "train_y shape: (1, 500)\n",
      "test_x_orig shape: (50, 28, 28)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 500)\n",
      "test_x's shape: (784, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    c = j.shape[1]\n",
    "    e = np.zeros((10, c))\n",
    "    y = 0\n",
    "    for column in j.T:\n",
    "        x = int(column)\n",
    "        e[x][y] = 1.0\n",
    "        y+=1\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y shape: (10, 500)\n",
      "test_y shape: (10, 50)\n"
     ]
    }
   ],
   "source": [
    "train_y = vectorize(train_y)\n",
    "test_y = vectorize(test_y)\n",
    "print(\"train_y shape: \" + str(train_y.shape))\n",
    "print(\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [784,30,10] #  2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "AL,cache = L_model_forward(train_x,parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.03, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.930760\n",
      "Cost after iteration 100: 3.267801\n",
      "Cost after iteration 200: 2.695543\n",
      "Cost after iteration 300: 1.896864\n",
      "Cost after iteration 400: 1.472211\n",
      "Cost after iteration 500: 1.176832\n",
      "Cost after iteration 600: 0.952356\n",
      "Cost after iteration 700: 0.786700\n",
      "Cost after iteration 800: 0.662374\n",
      "Cost after iteration 900: 0.567901\n",
      "Cost after iteration 1000: 0.494299\n",
      "Cost after iteration 1100: 0.434691\n",
      "Cost after iteration 1200: 0.385152\n",
      "Cost after iteration 1300: 0.343130\n",
      "Cost after iteration 1400: 0.306813\n",
      "Cost after iteration 1500: 0.275190\n",
      "Cost after iteration 1600: 0.247593\n",
      "Cost after iteration 1700: 0.223294\n",
      "Cost after iteration 1800: 0.201746\n",
      "Cost after iteration 1900: 0.182676\n",
      "Cost after iteration 2000: 0.165786\n",
      "Cost after iteration 2100: 0.150883\n",
      "Cost after iteration 2200: 0.137744\n",
      "Cost after iteration 2300: 0.126219\n",
      "Cost after iteration 2400: 0.116062\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X,Y,parameters):\n",
    "    AL, caches = L_model_forward(X, parameters)    \n",
    "    # Compute cost.\n",
    "    cost = compute_cost(AL, Y)\n",
    "    return cost,AL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of test data: 0.11401821900775506\n"
     ]
    }
   ],
   "source": [
    "cost , AL = test(test_x,test_y,parameters)\n",
    "print(\"Cost of test data: \"+str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(my_image,parameters):\n",
    "    AL, caches = L_model_forward(my_image, parameters)\n",
    "    res = np.max(AL,axis=0)\n",
    "    prediction = np.where(AL == res)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts number to be [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_image = \"image4.jpeg\" #Experiment with your own images. Images should be digits written in white with black background\n",
    "\n",
    "fname = \"images/\" + my_image #Put your image in images folder\n",
    "image = np.array(cv2.imread(fname, 0))\n",
    "my_image = misc.imresize(image, size=(28,28))/255\n",
    "my_imagef = my_image.reshape(28*28,1)\n",
    "plt.imshow(image)\n",
    "prediction= predict(my_imagef, parameters)\n",
    "print(\"The model predicts number to be \" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
